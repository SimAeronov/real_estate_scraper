from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions
from selenium.webdriver.chrome.options import Options

from bs4 import BeautifulSoup

############## HERE DATA SCRAPING STARTS ##############

# Config Chrome Driver
def config_driver(chrome_driver_path):
    ser = Service(chrome_driver_path)

    options = Options()
    options.page_load_strategy = 'none'
    driver = webdriver.Chrome(service=ser, options=options)

    return driver
def scrape_data_from_site(driver,link_to_site):
    driver.get(link_to_site)

    # Wait until first html image has loaded and get all data before cookies overlay
    WebDriverWait(driver, 1).until(expected_conditions.presence_of_element_located((By.TAG_NAME, "img")))
    html_text = driver.page_source
    driver.close()

    # Load saved html into a new BeautifulSoup object and get all data separately
    soup = BeautifulSoup(html_text, "html.parser")

    List_of_titles = soup.find_all(name="a", class_="lnk2")
    List_of_prices = soup.find_all(name="div", class_="price")

    # Organize Data and save to dict "List_collected_info"
    List_collected_info = []
    for element_titles_index in range(len(List_of_prices)):
        List_collected_info.append({"location": List_of_titles[element_titles_index].text,
                                    "price": List_of_prices[element_titles_index].text.replace(" EUR ", "").replace(" ",
                                                                                                                    ""),
                                    "link": List_of_titles[element_titles_index]["href"].replace("//", "")})

    # Find max number of pages and edit link +1 page until page = maxPage
    List_of_pages = soup.find_all(name="a", class_="pageNumbers")

    # Next I get the current page number, compare it to the current max number of pages.
    # If Current page < Max page, page += 1, else break
    index_page_substring = link_to_site.rfind("=")
    current_page_number = int(link_to_site[index_page_substring + 1:])
    max_page_number = int(List_of_pages[-2].text)

    if current_page_number <= max_page_number+1:
        link_to_site = link_to_site[:index_page_substring + 1] + str(current_page_number+1)
    print(f"current = {current_page_number}, max = {max_page_number}")

    return List_collected_info, link_to_site

############## HERE DATA SCRAPING ENDS, COULD ADD SORTING HERE W##############

# Sorting

############## WORK ON INPUTING DATA TO GOOGLE FORM NOW  ##############

def insert_form_data(chrome_driver_path,List_collected_info):
    ser = Service(chrome_driver_path)
    link_to_form = "https://docs.google.com/forms/d/e/1FAIpQLScPJTamqS0mkowc5KaAaIFqPwlNtKf4apK6hRH-eQ31T" \
                   "_fydw/viewform?usp=sf_link"

    op = webdriver.ChromeOptions().experimental_options
    driver = webdriver.Chrome(service=ser, options=op)
    driver.get(link_to_form)

    # Enter data "Location, Price, Link" inside Google Form [text boxes] which were found by their type of "text{
    for element_collected_data in List_collected_info:
        List_of_inputs = driver.find_elements(By.XPATH, "//input[@type='text']")
        List_of_inputs[0].send_keys(element_collected_data["location"])
        List_of_inputs[1].send_keys(element_collected_data["price"])
        List_of_inputs[2].send_keys(element_collected_data["link"])

        # First I click on "Submit Answer" then I click on "Submit new Answer"
        WebDriverWait(driver, 10).until(expected_conditions.visibility_of_element_located((By.XPATH,
                                                                                           "//*[text()='Изпращане']"))).click()
        WebDriverWait(driver, 10).until(
            expected_conditions.visibility_of_element_located((By.XPATH, "//*[text()='Изпращане на "
                                                                         "друг отговор']"))).click()

# Execute code from here
chrome_driver_path = "C://Users//Brock747//Desktop/UdemyC//ChromeDriver//chromedriver.exe"
link_to_site = r"https://www.imot.bg/pcgi/imot.cgi?act=3&slink=8giabx&f1=8"

List_collected_info = []
while True:
    driver = config_driver(chrome_driver_path)
    List_collected_info_new, link_to_site_new = scrape_data_from_site(driver,link_to_site)
    if link_to_site_new == link_to_site:
        print(f"Breking for {link_to_site_new} == {link_to_site}")
        break
    print(f"New Page Available {link_to_site_new}")
    link_to_site = link_to_site_new
    List_collected_info.extend(List_collected_info_new)



insert_form_data(chrome_driver_path,List_collected_info)

